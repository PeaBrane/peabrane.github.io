<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="dark">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>NVIDIA Dynamo: Distributed LLM Inference | Rudy Pei (裴彦儒)</title>
<meta name="keywords" content="llm-inference, distributed-systems, rust">
<meta name="description" content="Dynamo is NVIDIA&rsquo;s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k&#43; GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.">
<meta name="author" content="PeaBrane">
<link rel="canonical" href="https://peabrane.github.io/post/dynamo/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css" integrity="sha256-2jIR5e&#43;Ge/K3X9WmUVz&#43;1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://peabrane.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://peabrane.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://peabrane.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://peabrane.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://peabrane.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://peabrane.github.io/post/dynamo/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><meta property="og:url" content="https://peabrane.github.io/post/dynamo/">
  <meta property="og:site_name" content="Rudy Pei (裴彦儒)">
  <meta property="og:title" content="NVIDIA Dynamo: Distributed LLM Inference">
  <meta property="og:description" content="Dynamo is NVIDIA’s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k&#43; GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-10-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-01T00:00:00+00:00">
    <meta property="article:tag" content="Llm-Inference">
    <meta property="article:tag" content="Distributed-Systems">
    <meta property="article:tag" content="Rust">
      <meta property="og:image" content="https://peabrane.github.io/img/og.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://peabrane.github.io/img/og.png">
<meta name="twitter:title" content="NVIDIA Dynamo: Distributed LLM Inference">
<meta name="twitter:description" content="Dynamo is NVIDIA&rsquo;s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k&#43; GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "https://peabrane.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "NVIDIA Dynamo: Distributed LLM Inference",
      "item": "https://peabrane.github.io/post/dynamo/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "NVIDIA Dynamo: Distributed LLM Inference",
  "name": "NVIDIA Dynamo: Distributed LLM Inference",
  "description": "Dynamo is NVIDIA\u0026rsquo;s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k+ GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.\n",
  "keywords": [
    "llm-inference", "distributed-systems", "rust"
  ],
  "articleBody": "Dynamo is NVIDIA’s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k+ GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.\nI worked on three core components:\nKV-Aware Router — A hybrid router with two subsystems, the Indexer and the Slot Tracker, that together enable full KV-aware load balancing across workers. Engine Mockers — Lightweight engine simulators written entirely in Rust that replicate scheduling, block management, and timing behavior without requiring GPUs. Data Synthesizer — A trace-driven workload generator that learns the statistical structure of a real request trace and synthesizes new datasets with tunable knobs. Table of Contents KV-Aware Router Indexer Slot Tracker Event-Driven Updates Putting It Together: Full KV-Aware Load Balancing Engine Mockers Block Manager \u0026 Evictor Scheduler Performance Model Scalability Data Synthesizer Extracting the Core Radix Tree Transition Probabilities \u0026 Sampling Tunable Knobs Open Questions KV-Aware Router The router is a hybrid design built from two complementary components: the Indexer and the Slot Tracker. The Indexer tracks cached KV blocks across the fleet (the global picture), while the Slot Tracker tracks active KV blocks on each engine (the local picture). Together they give the router enough information to proxy for both the prefill cost and the decode cost of routing a request to any given worker.\nIndexer The Indexer maintains a global overview of all cached KV blocks across all backend workers. It is built on a radix tree, where each node in the tree corresponds to a block hash and is annotated with the set of workers that have that block cached. When a new request arrives, the router hashes its token sequence into block hashes and walks the radix tree to compute the prefix overlap (cache affinity) with each worker. This overlap directly tells us how many tokens the engine can skip during prefill — a longer overlap means a cheaper prefill.\nThe Indexer runs as a single-threaded actor: one dedicated thread owns the radix tree and processes all events (store, remove, match) sequentially through a channel. This keeps the data structure simple and lock-free — no concurrent access, no synchronization overhead. A multithreaded variant (the Flash Indexer) that relaxes this constraint for higher throughput is in the works — more on that in a future post.\nSlot Tracker The Slot Tracker (ActiveSequences / ActiveSequencesMultiWorker) tracks the active blocks currently held by each engine — i.e., the blocks that are in-flight for running requests. It maintains a per-worker view of which block hashes are alive (via reference-counted entries), how many prefill tokens are outstanding, and uses shared Rc pointers to deduplicate blocks that appear in multiple concurrent requests on the same worker.\nBecause active blocks are ephemeral (they exist only for the lifetime of a request), and because we can deterministically predict them from the request-response cycle (we know exactly when a request starts and ends), the Slot Tracker is updated locally by the router itself. There is no need to wait for the engine to tell us — we already know.\nThis local tracking also solves the thundering herd problem: if multiple router frontends all query a shared remote state for active load, they may simultaneously see the same “lightly loaded” worker and all route to it. By tracking active slots locally, each router maintains its own consistent view and naturally avoids this kind of pile-on.\nEvent-Driven Updates The Indexer is updated by engines communicating KV cache events via a pub/sub pattern (over NATS). This event-driven design is needed for two reasons:\nEviction opacity — The router cannot know when a cached block is evicted. Each engine has its own eviction policy (LRU, LFU, etc.), and eviction happens asynchronously inside the engine. The engine must tell us, because there is no other way to know.\nMulti-frontend consistency — In a deployment with multiple router replicas (frontends), the pub/sub fan-out ensures that all Indexers converge to the same global view. Every event is broadcast to all subscribers, so each frontend’s radix tree stays in sync by virtue of receiving the same stream of events.\nFor the Slot Tracker, local tracking is sufficient since the router controls the request lifecycle. However, we also support optional event-based syncing between routers (via ActiveSequenceEvent published over NATS) for deployments that want cross-replica consistency on active load as well.\nPutting It Together: Full KV-Aware Load Balancing The Indexer provides a proxy for the prefill cost: more cached prefix overlap with a worker means fewer new tokens to compute. The Slot Tracker provides a proxy for the decode cost: more active blocks on a worker means higher memory pressure and slower decode iterations (since decode is memory-bandwidth-bound). Together, these two signals allow the router to perform full KV-aware load balancing — routing each request to the worker that minimizes the total inference cost, not just naively round-robining or looking at queue depth.\nEngine Mockers Mockers are engine simulators that do not require actual GPU engines to work. They replicate the core behavior of an LLM inference engine — block management, scheduling, eviction, and timing — entirely in Rust, allowing you to test and benchmark the serving infrastructure (router, orchestrator, autoscaler) in isolation.\nBlock Manager \u0026 Evictor The KvManager is a synchronous block manager that mirrors the block lifecycle of a real engine. It maintains two pools: active blocks (in-use by running requests, with reference counts) and inactive blocks (evictable, managed by an LRU evictor). It processes four types of MoveBlock signals:\nUse — Allocate or re-activate a block: if already active, increment its refcount; if inactive, promote it back to active; if absent, evict the LRU-oldest inactive block to make room. Deref — Decrement a block’s reference count; when it hits zero, demote the block to the inactive pool. Destroy — Hard-remove a block from the active pool (e.g., on preemption). Promote — Convert a partial (generation-phase) block into a full block once it reaches block-size tokens. The evictor itself (LRUEvictor) uses a BTreeSet keyed by insertion counter to maintain strict LRU ordering, matching the behavior of vLLM’s evictor.\nThe current “manual” backend tracks reference counts explicitly via HashMap — functional, but not the most idiomatic Rust. An in-progress integration with KVBM (the KV Block Manager library) replaces this with an RAII-based block lifecycle: blocks are reference-counted via smart handles, and deallocation happens automatically when the last handle is dropped. Beyond cleaner code, this also opens the door to multi-tier memory simulation (e.g., GPU HBM + CPU DRAM + NVMe), since KVBM natively models tiered storage with configurable eviction strategies (Lineage, LRU, Multi-LRU).\nCrucially, the KvManager also publishes KV cache events (store / remove) to the same event sink that the real engines use. This means when a mocker evicts or allocates blocks, the Indexer in the router is updated exactly as it would be with a real engine — making the entire system testable end-to-end without GPUs.\nScheduler The Scheduler is an async scheduler that manages three queues — waiting, prefill, and decode — and drives the simulated forward pass loop:\nReceive new requests and place them in the waiting queue. Schedule waiting requests by checking resource budgets (watermark-based block budget, batched token budget, max sequence limit). If resources are insufficient, requests stay queued; if a running request must be preempted, the LRU-oldest decode request is evicted and re-enqueued. Simulate prefill — compute the prefill duration and advance blocks through the KvManager. Chunked prefill is supported: if the token budget is partially consumed, only a chunk is prefilled per iteration. Simulate decode — compute the decode duration based on active KV blocks, generate one output token per sequence, and check for completion or preemption. Performance Model Timing simulation is driven by a PerfModel with two variants:\nPolynomial (default) — Hardcoded polynomial formulas: prefill TTFT scales quadratically with new tokens (compute-bound), and decode ITL scales with active KV blocks (memory-bandwidth-bound). This aligns with the same heuristics the router uses for cost estimation.\nInterpolated — Load your own profiler sweeps from an NPZ file. The model builds 1D interpolation for prefill (ISL → TTFT) and 2D bilinear interpolation for decode (active KV tokens × context length → ITL). This lets you calibrate mockers to match real hardware profiles.\nScalability Since the mocker is fully in Rust with a thin Python CLI wrapper, each engine instance is just a tokio task — not a process, not a GPU. You can spin up 1000+ simulated engines on a single node (or more), making it practical to stress-test the router and orchestrator at datacenter-scale fleet sizes without any hardware.\nData Synthesizer The Data Synthesizer takes an existing hash trace (e.g., the Mooncake trace) and generates new synthetic datasets that preserve the statistical structure of the original while allowing controlled modifications via tunable knobs — in particular, prefix length multiplier and prefix root multiplier.\nExtracting the Core Radix Tree The crux of the synthesizer is figuring out what the prefixes are in a trace. This is straightforward: if a block hash appears more than once across requests, it’s part of a shared prefix. But we need more than just the set of prefixes — we need the statistical properties of the underlying radix tree.\nThe synthesizer builds a networkx DiGraph from the trace, where each node is a block hash and each edge records how many requests traversed it. It then:\nVerifies the tree property (no node has more than one parent). Marks leaf visits — nodes visited only once are unique user prompts, not shared context. Merges chains — contracts unary paths (chains of nodes with one predecessor and one successor) into single nodes with a length attribute, converting the prefix tree into a compact radix tree. This compression is important for efficient sampling. Removes leaves — strips the unique-visit nodes, leaving only the core radix tree of shared prefixes. The removed leaf lengths are saved as a separate empirical distribution for later sampling. Transition Probabilities \u0026 Sampling Each node in the core radix tree encodes its transition probability to children nodes via precomputed CDFs over edge weights. This means we can efficiently sample a path through the tree by walking from the super-root, sampling the next child at each node proportional to how frequently that transition appeared in the original trace, and stopping when we hit either an “end” sentinel (request terminates in the core tree) or a “cache end” sentinel (transition to a unique leaf path).\nThis assumes a kind of Markov property of request generation: the probability of visiting the next prefix block depends only on the current position in the tree, not the full history. For current workloads (multi-turn chat, document QA, few-shot prompting), this is a reasonable approximation.\nOnce the core path is sampled, the synthesizer appends a unique leaf path (user prompt) by sampling from the empirical distribution of leaf lengths extracted earlier. The input sequence length residual (tokens within the last partial block) and output sequence length are also sampled from their respective empirical distributions.\nTunable Knobs The synthesizer exposes several knobs for shaping the generated dataset, but the two most important are:\nprefix_len_multiplier — Scales the length of every node in the core radix tree. A multiplier of 2× doubles the number of shared context blocks per prefix, simulating workloads with longer system prompts or longer document contexts. prefix_root_multiplier — Replicates the entire core radix tree N times (with offset hash IDs). A multiplier of 4× means 4 independent prefix families, simulating a deployment serving 4× as many distinct applications or tenants. Additional knobs include prompt_len_multiplier (scales unique user prompt lengths), osl_multiplier (scales output sequence lengths), and speedup_ratio (compresses inter-request arrival times).\nOpen Questions The Markovian assumption works well for today’s workloads, but may need revisiting as more complex agentic workflows emerge — agent-subagent hierarchies, parallel tool calls, swarm architectures, etc. These patterns could introduce long-range dependencies in the prefix tree (e.g., an agent’s prefix depends on the outcome of a subagent call three levels deep) that the current memoryless model wouldn’t capture. There are many open statistical questions here around how to model the request-generation process for these more structured and dynamic workloads.\nBack to projects\n",
  "wordCount" : "2045",
  "inLanguage": "en",
  "image": "https://peabrane.github.io/img/og.png","datePublished": "2025-10-01T00:00:00Z",
  "dateModified": "2025-10-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "PeaBrane"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://peabrane.github.io/post/dynamo/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Rudy Pei (裴彦儒)",
    "logo": {
      "@type": "ImageObject",
      "url": "https://peabrane.github.io/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://peabrane.github.io/" accesskey="h" title="Rudy Pei (裴彦儒) (Alt + H)">Rudy Pei (裴彦儒)</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://peabrane.github.io/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://peabrane.github.io/post/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://peabrane.github.io/comps/" title="Music">
                    <span>Music</span>
                </a>
            </li>
            <li>
                <a href="https://peabrane.github.io/poems/" title="Poems">
                    <span>Poems</span>
                </a>
            </li>
            <li>
                <a href="https://peabrane.github.io/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
            <li>
                <a href="https://peabrane.github.io/links/" title="Links">
                    <span>Links</span>
                </a>
            </li>
            <li>
                <a href="https://peabrane.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://peabrane.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://peabrane.github.io/post/">Projects</a></div>
    <h1 class="post-title entry-hint-parent">
      NVIDIA Dynamo: Distributed LLM Inference
    </h1>
    <div class="post-meta"><span title='2025-10-01 00:00:00 +0000 UTC'>October 1, 2025</span>&nbsp;·&nbsp;<span>10 min</span>&nbsp;·&nbsp;<span>2045 words</span>&nbsp;·&nbsp;<span>PeaBrane</span>

</div>
  </header> 
  <div class="post-content"><p><a href="https://github.com/ai-dynamo/dynamo">Dynamo</a> is NVIDIA&rsquo;s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k+ GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.</p>
<p>I worked on three core components:</p>
<ul>
<li><a href="#kv-aware-router"><strong>KV-Aware Router</strong></a> — A hybrid router with two subsystems, the <strong>Indexer</strong> and the <strong>Slot Tracker</strong>, that together enable full KV-aware load balancing across workers.</li>
<li><a href="#engine-mockers"><strong>Engine Mockers</strong></a> — Lightweight engine simulators written entirely in Rust that replicate scheduling, block management, and timing behavior without requiring GPUs.</li>
<li><a href="#data-synthesizer"><strong>Data Synthesizer</strong></a> — A trace-driven workload generator that learns the statistical structure of a real request trace and synthesizes new datasets with tunable knobs.</li>
</ul>
<hr>
<h2 id="table-of-contents">Table of Contents<a hidden class="anchor" aria-hidden="true" href="#table-of-contents">#</a></h2>
<ol>
<li><a href="#kv-aware-router">KV-Aware Router</a>
<ul>
<li><a href="#indexer">Indexer</a></li>
<li><a href="#slot-tracker">Slot Tracker</a></li>
<li><a href="#event-driven-updates">Event-Driven Updates</a></li>
<li><a href="#putting-it-together-full-kv-aware-load-balancing">Putting It Together: Full KV-Aware Load Balancing</a></li>
</ul>
</li>
<li><a href="#engine-mockers">Engine Mockers</a>
<ul>
<li><a href="#block-manager--evictor">Block Manager &amp; Evictor</a></li>
<li><a href="#scheduler">Scheduler</a></li>
<li><a href="#performance-model">Performance Model</a></li>
<li><a href="#scalability">Scalability</a></li>
</ul>
</li>
<li><a href="#data-synthesizer">Data Synthesizer</a>
<ul>
<li><a href="#extracting-the-core-radix-tree">Extracting the Core Radix Tree</a></li>
<li><a href="#transition-probabilities--sampling">Transition Probabilities &amp; Sampling</a></li>
<li><a href="#tunable-knobs">Tunable Knobs</a></li>
<li><a href="#open-questions">Open Questions</a></li>
</ul>
</li>
</ol>
<hr>
<h2 id="kv-aware-router">KV-Aware Router<a hidden class="anchor" aria-hidden="true" href="#kv-aware-router">#</a></h2>
<p>The router is a <strong>hybrid design</strong> built from two complementary components: the <strong>Indexer</strong> and the <strong>Slot Tracker</strong>. The Indexer tracks <em>cached</em> KV blocks across the fleet (the global picture), while the Slot Tracker tracks <em>active</em> KV blocks on each engine (the local picture). Together they give the router enough information to proxy for both the prefill cost and the decode cost of routing a request to any given worker.</p>
<h3 id="indexer">Indexer<a hidden class="anchor" aria-hidden="true" href="#indexer">#</a></h3>
<p>The Indexer maintains a <strong>global overview of all cached KV blocks across all backend workers</strong>. It is built on a <a href="https://en.wikipedia.org/wiki/Radix_tree">radix tree</a>, where each node in the tree corresponds to a block hash and is annotated with the set of workers that have that block cached. When a new request arrives, the router hashes its token sequence into block hashes and walks the radix tree to compute the <strong>prefix overlap</strong> (cache affinity) with each worker. This overlap directly tells us how many tokens the engine can skip during prefill — a longer overlap means a cheaper prefill.</p>
<p>The Indexer runs as a single-threaded <a href="https://en.wikipedia.org/wiki/Actor_model">actor</a>: one dedicated thread owns the radix tree and processes all events (store, remove, match) sequentially through a channel. This keeps the data structure simple and lock-free — no concurrent access, no synchronization overhead. A multithreaded variant (the <em>Flash Indexer</em>) that relaxes this constraint for higher throughput is in the works — more on that in a future post.</p>
<h3 id="slot-tracker">Slot Tracker<a hidden class="anchor" aria-hidden="true" href="#slot-tracker">#</a></h3>
<p>The Slot Tracker (<code>ActiveSequences</code> / <code>ActiveSequencesMultiWorker</code>) tracks the <strong>active blocks currently held by each engine</strong> — i.e., the blocks that are in-flight for running requests. It maintains a per-worker view of which block hashes are alive (via reference-counted entries), how many prefill tokens are outstanding, and uses shared <code>Rc</code> pointers to deduplicate blocks that appear in multiple concurrent requests on the same worker.</p>
<p>Because active blocks are <strong>ephemeral</strong> (they exist only for the lifetime of a request), and because we can deterministically predict them from the request-response cycle (we know exactly when a request starts and ends), the Slot Tracker is updated <strong>locally</strong> by the router itself. There is no need to wait for the engine to tell us — we already know.</p>
<p>This local tracking also solves the <strong><a href="https://en.wikipedia.org/wiki/Thundering_herd_problem">thundering herd problem</a></strong>: if multiple router frontends all query a shared remote state for active load, they may simultaneously see the same &ldquo;lightly loaded&rdquo; worker and all route to it. By tracking active slots locally, each router maintains its own consistent view and naturally avoids this kind of pile-on.</p>
<h3 id="event-driven-updates">Event-Driven Updates<a hidden class="anchor" aria-hidden="true" href="#event-driven-updates">#</a></h3>
<p>The Indexer is updated by engines communicating <strong>KV cache events</strong> via a <strong><a href="https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern">pub/sub pattern</a></strong> (over <a href="https://nats.io/">NATS</a>). This event-driven design is needed for two reasons:</p>
<ol>
<li>
<p><strong>Eviction opacity</strong> — The router cannot know when a cached block is evicted. Each engine has its own <a href="https://en.wikipedia.org/wiki/Cache_replacement_policies">eviction policy</a> (LRU, LFU, etc.), and eviction happens asynchronously inside the engine. The engine <em>must</em> tell us, because there is no other way to know.</p>
</li>
<li>
<p><strong>Multi-frontend consistency</strong> — In a deployment with multiple router replicas (frontends), the pub/sub fan-out ensures that all Indexers converge to the same global view. Every event is broadcast to all subscribers, so each frontend&rsquo;s radix tree stays in sync by virtue of receiving the same stream of events.</p>
</li>
</ol>
<p>For the Slot Tracker, local tracking is sufficient since the router controls the request lifecycle. However, we also support optional <strong>event-based syncing between routers</strong> (via <code>ActiveSequenceEvent</code> published over <a href="https://nats.io/">NATS</a>) for deployments that want cross-replica consistency on active load as well.</p>
<h3 id="putting-it-together-full-kv-aware-load-balancing">Putting It Together: Full KV-Aware Load Balancing<a hidden class="anchor" aria-hidden="true" href="#putting-it-together-full-kv-aware-load-balancing">#</a></h3>
<p>The Indexer provides a proxy for the <strong>prefill cost</strong>: more cached prefix overlap with a worker means fewer new tokens to compute. The Slot Tracker provides a proxy for the <strong>decode cost</strong>: more active blocks on a worker means higher memory pressure and slower decode iterations (since decode is memory-bandwidth-bound). Together, these two signals allow the router to perform <strong>full KV-aware load balancing</strong> — routing each request to the worker that minimizes the total inference cost, not just naively round-robining or looking at queue depth.</p>
<hr>
<h2 id="engine-mockers">Engine Mockers<a hidden class="anchor" aria-hidden="true" href="#engine-mockers">#</a></h2>
<p>Mockers are <strong>engine simulators</strong> that do not require actual GPU engines to work. They replicate the core behavior of an LLM inference engine — block management, scheduling, eviction, and timing — entirely in Rust, allowing you to test and benchmark the serving infrastructure (router, orchestrator, autoscaler) in isolation.</p>
<h3 id="block-manager--evictor">Block Manager &amp; Evictor<a hidden class="anchor" aria-hidden="true" href="#block-manager--evictor">#</a></h3>
<p>The <code>KvManager</code> is a synchronous block manager that mirrors the block lifecycle of a real engine. It maintains two pools: <strong>active blocks</strong> (in-use by running requests, with reference counts) and <strong>inactive blocks</strong> (evictable, managed by an LRU evictor). It processes four types of <code>MoveBlock</code> signals:</p>
<ul>
<li><strong>Use</strong> — Allocate or re-activate a block: if already active, increment its refcount; if inactive, promote it back to active; if absent, evict the LRU-oldest inactive block to make room.</li>
<li><strong>Deref</strong> — Decrement a block&rsquo;s reference count; when it hits zero, demote the block to the inactive pool.</li>
<li><strong>Destroy</strong> — Hard-remove a block from the active pool (e.g., on preemption).</li>
<li><strong>Promote</strong> — Convert a partial (generation-phase) block into a full block once it reaches block-size tokens.</li>
</ul>
<p>The evictor itself (<code>LRUEvictor</code>) uses a <code>BTreeSet</code> keyed by insertion counter to maintain strict LRU ordering, matching the behavior of <a href="https://github.com/vllm-project/vllm">vLLM</a>&rsquo;s evictor.</p>
<p>The current &ldquo;manual&rdquo; backend tracks reference counts explicitly via <code>HashMap&lt;UniqueBlock, usize&gt;</code> — functional, but not the most idiomatic Rust. An in-progress integration with <strong>KVBM</strong> (the KV Block Manager library) replaces this with an <a href="https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization">RAII</a>-based block lifecycle: blocks are reference-counted via smart handles, and deallocation happens automatically when the last handle is dropped. Beyond cleaner code, this also opens the door to <strong>multi-tier memory simulation</strong> (e.g., GPU HBM + CPU DRAM + NVMe), since KVBM natively models tiered storage with configurable eviction strategies (Lineage, LRU, Multi-LRU).</p>
<p>Crucially, the KvManager also <strong>publishes KV cache events</strong> (store / remove) to the same event sink that the real engines use. This means when a mocker evicts or allocates blocks, the Indexer in the router is updated exactly as it would be with a real engine — making the entire system testable end-to-end without GPUs.</p>
<h3 id="scheduler">Scheduler<a hidden class="anchor" aria-hidden="true" href="#scheduler">#</a></h3>
<p>The <code>Scheduler</code> is an async scheduler that manages three queues — waiting, prefill, and decode — and drives the simulated forward pass loop:</p>
<ol>
<li><strong>Receive</strong> new requests and place them in the waiting queue.</li>
<li><strong>Schedule</strong> waiting requests by checking resource budgets (watermark-based block budget, batched token budget, max sequence limit). If resources are insufficient, requests stay queued; if a running request must be preempted, the LRU-oldest decode request is evicted and re-enqueued.</li>
<li><strong>Simulate prefill</strong> — compute the prefill duration and advance blocks through the KvManager. Chunked prefill is supported: if the token budget is partially consumed, only a chunk is prefilled per iteration.</li>
<li><strong>Simulate decode</strong> — compute the decode duration based on active KV blocks, generate one output token per sequence, and check for completion or preemption.</li>
</ol>
<h3 id="performance-model">Performance Model<a hidden class="anchor" aria-hidden="true" href="#performance-model">#</a></h3>
<p>Timing simulation is driven by a <code>PerfModel</code> with two variants:</p>
<ul>
<li>
<p><strong>Polynomial (default)</strong> — Hardcoded polynomial formulas: prefill TTFT scales quadratically with new tokens <em>(compute-bound)</em>, and decode ITL scales with active KV blocks <em>(memory-bandwidth-bound)</em>. This aligns with the same heuristics the router uses for cost estimation.</p>
</li>
<li>
<p><strong>Interpolated</strong> — Load your own profiler sweeps from an NPZ file. The model builds 1D interpolation for prefill (ISL → TTFT) and 2D <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation">bilinear interpolation</a> for decode (active KV tokens × context length → ITL). This lets you calibrate mockers to match real hardware profiles.</p>
</li>
</ul>
<h3 id="scalability">Scalability<a hidden class="anchor" aria-hidden="true" href="#scalability">#</a></h3>
<p>Since the mocker is fully in Rust with a thin Python CLI wrapper, each engine instance is just a <strong>tokio task</strong> — not a process, not a GPU. You can spin up <strong>1000+ simulated engines on a single node</strong> (or more), making it practical to stress-test the router and orchestrator at datacenter-scale fleet sizes without any hardware.</p>
<hr>
<h2 id="data-synthesizer">Data Synthesizer<a hidden class="anchor" aria-hidden="true" href="#data-synthesizer">#</a></h2>
<p>The Data Synthesizer takes an existing <strong>hash trace</strong> (e.g., the Mooncake trace) and generates new synthetic datasets that preserve the statistical structure of the original while allowing controlled modifications via tunable knobs — in particular, <strong>prefix length multiplier</strong> and <strong>prefix root multiplier</strong>.</p>
<h3 id="extracting-the-core-radix-tree">Extracting the Core Radix Tree<a hidden class="anchor" aria-hidden="true" href="#extracting-the-core-radix-tree">#</a></h3>
<p>The crux of the synthesizer is figuring out what the <em>prefixes</em> are in a trace. This is straightforward: if a block hash appears more than once across requests, it&rsquo;s part of a shared prefix. But we need more than just the set of prefixes — we need the <strong>statistical properties of the underlying radix tree</strong>.</p>
<p>The synthesizer builds a <code>networkx</code> DiGraph from the trace, where each node is a block hash and each edge records how many requests traversed it. It then:</p>
<ol>
<li><strong>Verifies</strong> the tree property (no node has more than one parent).</li>
<li><strong>Marks</strong> leaf visits — nodes visited only once are unique user prompts, not shared context.</li>
<li><strong>Merges chains</strong> — contracts unary paths (chains of nodes with one predecessor and one successor) into single nodes with a <code>length</code> attribute, converting the prefix tree into a compact <strong><a href="https://en.wikipedia.org/wiki/Radix_tree">radix tree</a></strong>. This compression is important for efficient sampling.</li>
<li><strong>Removes leaves</strong> — strips the unique-visit nodes, leaving only the <strong>core radix tree</strong> of shared prefixes. The removed leaf lengths are saved as a separate empirical distribution for later sampling.</li>
</ol>
<h3 id="transition-probabilities--sampling">Transition Probabilities &amp; Sampling<a hidden class="anchor" aria-hidden="true" href="#transition-probabilities--sampling">#</a></h3>
<p>Each node in the core radix tree encodes its <strong>transition probability to children nodes</strong> via precomputed <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">CDFs</a> over edge weights. This means we can efficiently sample a path through the tree by walking from the super-root, sampling the next child at each node proportional to how frequently that transition appeared in the original trace, and stopping when we hit either an &ldquo;end&rdquo; sentinel (request terminates in the core tree) or a &ldquo;cache end&rdquo; sentinel (transition to a unique leaf path).</p>
<p>This assumes a kind of <strong><a href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a></strong> of request generation: the probability of visiting the next prefix block depends only on the current position in the tree, not the full history. For current workloads (multi-turn chat, document QA, few-shot prompting), this is a reasonable approximation.</p>
<p>Once the core path is sampled, the synthesizer appends a <strong>unique leaf path</strong> (user prompt) by sampling from the empirical distribution of leaf lengths extracted earlier. The input sequence length residual (tokens within the last partial block) and output sequence length are also sampled from their respective empirical distributions.</p>
<h3 id="tunable-knobs">Tunable Knobs<a hidden class="anchor" aria-hidden="true" href="#tunable-knobs">#</a></h3>
<p>The synthesizer exposes several knobs for shaping the generated dataset, but the two most important are:</p>
<ul>
<li><strong><code>prefix_len_multiplier</code></strong> — Scales the length of every node in the core radix tree. A multiplier of 2× doubles the number of shared context blocks per prefix, simulating workloads with longer system prompts or longer document contexts.</li>
<li><strong><code>prefix_root_multiplier</code></strong> — Replicates the entire core radix tree N times (with offset hash IDs). A multiplier of 4× means 4 independent prefix families, simulating a deployment serving 4× as many distinct applications or tenants.</li>
</ul>
<p>Additional knobs include <code>prompt_len_multiplier</code> (scales unique user prompt lengths), <code>osl_multiplier</code> (scales output sequence lengths), and <code>speedup_ratio</code> (compresses inter-request arrival times).</p>
<h3 id="open-questions">Open Questions<a hidden class="anchor" aria-hidden="true" href="#open-questions">#</a></h3>
<p>The <a href="https://en.wikipedia.org/wiki/Markov_property">Markovian</a> assumption works well for today&rsquo;s workloads, but may need revisiting as more complex <strong>agentic workflows</strong> emerge — agent-subagent hierarchies, parallel tool calls, swarm architectures, etc. These patterns could introduce long-range dependencies in the prefix tree (e.g., an agent&rsquo;s prefix depends on the outcome of a subagent call three levels deep) that the current memoryless model wouldn&rsquo;t capture. There are many open statistical questions here around how to model the request-generation process for these more structured and dynamic workloads.</p>
<hr>
<p><a href="/post">Back to projects</a></p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://peabrane.github.io/tags/llm-inference/">Llm-Inference</a></li>
      <li><a href="https://peabrane.github.io/tags/distributed-systems/">Distributed-Systems</a></li>
      <li><a href="https://peabrane.github.io/tags/rust/">Rust</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://peabrane.github.io/post/tenns/">
    <span class="title">Next »</span>
    <br>
    <span>Temporal Neural Networks (TENNs) at BrainChip</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://peabrane.github.io/">Rudy Pei (裴彦儒)</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
