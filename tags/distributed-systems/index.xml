<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Distributed-Systems on Rudy Pei (裴彦儒)</title>
    <link>https://peabrane.github.io/tags/distributed-systems/</link>
    <description>Recent content in Distributed-Systems on Rudy Pei (裴彦儒)</description>
    <image>
      <title>Rudy Pei (裴彦儒)</title>
      <url>https://peabrane.github.io/img/og.png</url>
      <link>https://peabrane.github.io/img/og.png</link>
    </image>
    <generator>Hugo -- 0.155.3</generator>
    <language>en</language>
    <lastBuildDate>Wed, 01 Oct 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://peabrane.github.io/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NVIDIA Dynamo: Distributed LLM Inference</title>
      <link>https://peabrane.github.io/post/dynamo/</link>
      <pubDate>Wed, 01 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://peabrane.github.io/post/dynamo/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/ai-dynamo/dynamo&#34;&gt;Dynamo&lt;/a&gt; is NVIDIA&amp;rsquo;s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k+ GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
