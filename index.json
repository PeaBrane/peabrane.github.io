[{"content":"Simulating spin glasses at scale has always been a pain — Python is too slow, and C++ is too painful. Peapods is my attempt at a middle ground: a Monte Carlo package for Ising spin systems where the hot loops are written in Rust and everything else stays in Python, glued together with PyO3. You just pip install peapods and go.\nThe package supports the usual suspects — Metropolis and Gibbs single-spin flips, Swendsen–Wang and Wolff cluster updates, and parallel tempering — on arbitrary-dimensional hypercubic lattices with arbitrary coupling constants. Replicas run in parallel via Rayon, so you get multi-core scaling essentially for free.\nThe whole thing started because I needed a fast, flexible simulator for my spin glass phase transition work and got tired of fighting with Numba. Rust turned out to be a surprisingly pleasant language for this kind of thing.\nBack to projects\n","permalink":"https://peabrane.github.io/post/peapods/","summary":"\u003cp\u003eSimulating \u003ca href=\"https://en.wikipedia.org/wiki/Spin_glass\"\u003espin glasses\u003c/a\u003e at scale has always been a pain — Python is too slow, and C++ is too painful. \u003ca href=\"https://github.com/PeaBrane/peapods\"\u003ePeapods\u003c/a\u003e is my attempt at a middle ground: a Monte Carlo package for Ising spin systems where the hot loops are written in Rust and everything else stays in Python, glued together with \u003ca href=\"https://pyo3.rs\"\u003ePyO3\u003c/a\u003e. You just \u003ccode\u003epip install peapods\u003c/code\u003e and go.\u003c/p\u003e","title":"Peapods: Ising Monte Carlo in Rust"},{"content":"Dynamo is NVIDIA\u0026rsquo;s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k+ GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.\nI worked on three core components:\nKV-Aware Router — A hybrid router with two subsystems, the Indexer and the Slot Tracker, that together enable full KV-aware load balancing across workers. Engine Mockers — Lightweight engine simulators written entirely in Rust that replicate scheduling, block management, and timing behavior without requiring GPUs. Data Synthesizer — A trace-driven workload generator that learns the statistical structure of a real request trace and synthesizes new datasets with tunable knobs. Table of Contents KV-Aware Router Indexer Slot Tracker Event-Driven Updates Putting It Together: Full KV-Aware Load Balancing Engine Mockers Block Manager \u0026amp; Evictor Scheduler Performance Model Scalability Data Synthesizer Extracting the Core Radix Tree Transition Probabilities \u0026amp; Sampling Tunable Knobs Open Questions KV-Aware Router The router is a hybrid design built from two complementary components: the Indexer and the Slot Tracker. The Indexer tracks cached KV blocks across the fleet (the global picture), while the Slot Tracker tracks active KV blocks on each engine (the local picture). Together they give the router enough information to proxy for both the prefill cost and the decode cost of routing a request to any given worker.\nIndexer The Indexer maintains a global overview of all cached KV blocks across all backend workers. It is built on a radix tree, where each node in the tree corresponds to a block hash and is annotated with the set of workers that have that block cached. When a new request arrives, the router hashes its token sequence into block hashes and walks the radix tree to compute the prefix overlap (cache affinity) with each worker. This overlap directly tells us how many tokens the engine can skip during prefill — a longer overlap means a cheaper prefill.\nThe Indexer runs as a single-threaded actor: one dedicated thread owns the radix tree and processes all events (store, remove, match) sequentially through a channel. This keeps the data structure simple and lock-free — no concurrent access, no synchronization overhead. A multithreaded variant (the Flash Indexer) that relaxes this constraint for higher throughput is in the works — more on that in a future post.\nSlot Tracker The Slot Tracker (ActiveSequences / ActiveSequencesMultiWorker) tracks the active blocks currently held by each engine — i.e., the blocks that are in-flight for running requests. It maintains a per-worker view of which block hashes are alive (via reference-counted entries), how many prefill tokens are outstanding, and uses shared Rc pointers to deduplicate blocks that appear in multiple concurrent requests on the same worker.\nBecause active blocks are ephemeral (they exist only for the lifetime of a request), and because we can deterministically predict them from the request-response cycle (we know exactly when a request starts and ends), the Slot Tracker is updated locally by the router itself. There is no need to wait for the engine to tell us — we already know.\nThis local tracking also solves the thundering herd problem: if multiple router frontends all query a shared remote state for active load, they may simultaneously see the same \u0026ldquo;lightly loaded\u0026rdquo; worker and all route to it. By tracking active slots locally, each router maintains its own consistent view and naturally avoids this kind of pile-on.\nEvent-Driven Updates The Indexer is updated by engines communicating KV cache events via a pub/sub pattern (over NATS). This event-driven design is needed for two reasons:\nEviction opacity — The router cannot know when a cached block is evicted. Each engine has its own eviction policy (LRU, LFU, etc.), and eviction happens asynchronously inside the engine. The engine must tell us, because there is no other way to know.\nMulti-frontend consistency — In a deployment with multiple router replicas (frontends), the pub/sub fan-out ensures that all Indexers converge to the same global view. Every event is broadcast to all subscribers, so each frontend\u0026rsquo;s radix tree stays in sync by virtue of receiving the same stream of events.\nFor the Slot Tracker, local tracking is sufficient since the router controls the request lifecycle. However, we also support optional event-based syncing between routers (via ActiveSequenceEvent published over NATS) for deployments that want cross-replica consistency on active load as well.\nPutting It Together: Full KV-Aware Load Balancing The Indexer provides a proxy for the prefill cost: more cached prefix overlap with a worker means fewer new tokens to compute. The Slot Tracker provides a proxy for the decode cost: more active blocks on a worker means higher memory pressure and slower decode iterations (since decode is memory-bandwidth-bound). Together, these two signals allow the router to perform full KV-aware load balancing — routing each request to the worker that minimizes the total inference cost, not just naively round-robining or looking at queue depth.\nEngine Mockers Mockers are engine simulators that do not require actual GPU engines to work. They replicate the core behavior of an LLM inference engine — block management, scheduling, eviction, and timing — entirely in Rust, allowing you to test and benchmark the serving infrastructure (router, orchestrator, autoscaler) in isolation.\nBlock Manager \u0026amp; Evictor The KvManager is a synchronous block manager that mirrors the block lifecycle of a real engine. It maintains two pools: active blocks (in-use by running requests, with reference counts) and inactive blocks (evictable, managed by an LRU evictor). It processes four types of MoveBlock signals:\nUse — Allocate or re-activate a block: if already active, increment its refcount; if inactive, promote it back to active; if absent, evict the LRU-oldest inactive block to make room. Deref — Decrement a block\u0026rsquo;s reference count; when it hits zero, demote the block to the inactive pool. Destroy — Hard-remove a block from the active pool (e.g., on preemption). Promote — Convert a partial (generation-phase) block into a full block once it reaches block-size tokens. The evictor itself (LRUEvictor) uses a BTreeSet keyed by insertion counter to maintain strict LRU ordering, matching the behavior of vLLM\u0026rsquo;s evictor.\nThe current \u0026ldquo;manual\u0026rdquo; backend tracks reference counts explicitly via HashMap\u0026lt;UniqueBlock, usize\u0026gt; — functional, but not the most idiomatic Rust. An in-progress integration with KVBM (the KV Block Manager library) replaces this with an RAII-based block lifecycle: blocks are reference-counted via smart handles, and deallocation happens automatically when the last handle is dropped. Beyond cleaner code, this also opens the door to multi-tier memory simulation (e.g., GPU HBM + CPU DRAM + NVMe), since KVBM natively models tiered storage with configurable eviction strategies (Lineage, LRU, Multi-LRU).\nCrucially, the KvManager also publishes KV cache events (store / remove) to the same event sink that the real engines use. This means when a mocker evicts or allocates blocks, the Indexer in the router is updated exactly as it would be with a real engine — making the entire system testable end-to-end without GPUs.\nScheduler The Scheduler is an async scheduler that manages three queues — waiting, prefill, and decode — and drives the simulated forward pass loop:\nReceive new requests and place them in the waiting queue. Schedule waiting requests by checking resource budgets (watermark-based block budget, batched token budget, max sequence limit). If resources are insufficient, requests stay queued; if a running request must be preempted, the LRU-oldest decode request is evicted and re-enqueued. Simulate prefill — compute the prefill duration and advance blocks through the KvManager. Chunked prefill is supported: if the token budget is partially consumed, only a chunk is prefilled per iteration. Simulate decode — compute the decode duration based on active KV blocks, generate one output token per sequence, and check for completion or preemption. Performance Model Timing simulation is driven by a PerfModel with two variants:\nPolynomial (default) — Hardcoded polynomial formulas: prefill TTFT scales quadratically with new tokens (compute-bound), and decode ITL scales with active KV blocks (memory-bandwidth-bound). This aligns with the same heuristics the router uses for cost estimation.\nInterpolated — Load your own profiler sweeps from an NPZ file. The model builds 1D interpolation for prefill (ISL → TTFT) and 2D bilinear interpolation for decode (active KV tokens × context length → ITL). This lets you calibrate mockers to match real hardware profiles.\nScalability Since the mocker is fully in Rust with a thin Python CLI wrapper, each engine instance is just a tokio task — not a process, not a GPU. You can spin up 1000+ simulated engines on a single node (or more), making it practical to stress-test the router and orchestrator at datacenter-scale fleet sizes without any hardware.\nData Synthesizer The Data Synthesizer takes an existing hash trace (e.g., the Mooncake trace) and generates new synthetic datasets that preserve the statistical structure of the original while allowing controlled modifications via tunable knobs — in particular, prefix length multiplier and prefix root multiplier.\nExtracting the Core Radix Tree The crux of the synthesizer is figuring out what the prefixes are in a trace. This is straightforward: if a block hash appears more than once across requests, it\u0026rsquo;s part of a shared prefix. But we need more than just the set of prefixes — we need the statistical properties of the underlying radix tree.\nThe synthesizer builds a networkx DiGraph from the trace, where each node is a block hash and each edge records how many requests traversed it. It then:\nVerifies the tree property (no node has more than one parent). Marks leaf visits — nodes visited only once are unique user prompts, not shared context. Merges chains — contracts unary paths (chains of nodes with one predecessor and one successor) into single nodes with a length attribute, converting the prefix tree into a compact radix tree. This compression is important for efficient sampling. Removes leaves — strips the unique-visit nodes, leaving only the core radix tree of shared prefixes. The removed leaf lengths are saved as a separate empirical distribution for later sampling. Transition Probabilities \u0026amp; Sampling Each node in the core radix tree encodes its transition probability to children nodes via precomputed CDFs over edge weights. This means we can efficiently sample a path through the tree by walking from the super-root, sampling the next child at each node proportional to how frequently that transition appeared in the original trace, and stopping when we hit either an \u0026ldquo;end\u0026rdquo; sentinel (request terminates in the core tree) or a \u0026ldquo;cache end\u0026rdquo; sentinel (transition to a unique leaf path).\nThis assumes a kind of Markov property of request generation: the probability of visiting the next prefix block depends only on the current position in the tree, not the full history. For current workloads (multi-turn chat, document QA, few-shot prompting), this is a reasonable approximation.\nOnce the core path is sampled, the synthesizer appends a unique leaf path (user prompt) by sampling from the empirical distribution of leaf lengths extracted earlier. The input sequence length residual (tokens within the last partial block) and output sequence length are also sampled from their respective empirical distributions.\nTunable Knobs The synthesizer exposes several knobs for shaping the generated dataset, but the two most important are:\nprefix_len_multiplier — Scales the length of every node in the core radix tree. A multiplier of 2× doubles the number of shared context blocks per prefix, simulating workloads with longer system prompts or longer document contexts. prefix_root_multiplier — Replicates the entire core radix tree N times (with offset hash IDs). A multiplier of 4× means 4 independent prefix families, simulating a deployment serving 4× as many distinct applications or tenants. Additional knobs include prompt_len_multiplier (scales unique user prompt lengths), osl_multiplier (scales output sequence lengths), and speedup_ratio (compresses inter-request arrival times).\nOpen Questions The Markovian assumption works well for today\u0026rsquo;s workloads, but may need revisiting as more complex agentic workflows emerge — agent-subagent hierarchies, parallel tool calls, swarm architectures, etc. These patterns could introduce long-range dependencies in the prefix tree (e.g., an agent\u0026rsquo;s prefix depends on the outcome of a subagent call three levels deep) that the current memoryless model wouldn\u0026rsquo;t capture. There are many open statistical questions here around how to model the request-generation process for these more structured and dynamic workloads.\nBack to projects\n","permalink":"https://peabrane.github.io/post/dynamo/","summary":"\u003cp\u003e\u003ca href=\"https://github.com/ai-dynamo/dynamo\"\u003eDynamo\u003c/a\u003e is NVIDIA\u0026rsquo;s open-source datacenter-scale distributed inference serving framework for generative AI and reasoning models. Built in Rust for performance and Python for extensibility, it supports disaggregated prefill and decode, dynamic GPU scheduling, and LLM-aware request routing across multi-node multi-GPU topologies. The project has 6k+ GitHub stars and supports backends including TensorRT-LLM, vLLM, and SGLang.\u003c/p\u003e","title":"NVIDIA Dynamo: Distributed LLM Inference"},{"content":"While working at BrainChip, I led the research and software-hardware codesign of TENNs (Temporal Neural Networks) — a family of deep state-space models designed for efficient, low-latency inference on edge hardware. This line of work developed in parallel with the broader SSM wave (Mamba, Gated Delta Net, etc.), cross-pollinating ideas around structured recurrences, parallel scans, and kernel parameterizations.\nThe project resulted in several publications:\nTENNsEye (CVPR 2024 Workshop) — A lightweight causal spatiotemporal network for online eye tracking with event cameras. Targets edge hardware via simple operations (convolutions, ReLU), online inference through output buffering, and \u0026gt;90% activation sparsity through regularization. Reached 0.9916 p10 accuracy on the AIS 2024 challenge.\nCentaurus (ICLR 2025 Spotlight, solo author) — Treats SSM operations as tensor contractions and systematically determines their optimal ordering to maximize training efficiency. This unlocks flexible SSM block designs beyond depthwise-separable configurations — group convolutions, full convolutions, bottleneck blocks — all composed into a heterogeneous architecture. The first fully state-space network with competitive ASR performance without LSTMs, explicit convolutions, or attention.\naTENNuate (Interspeech 2025) — A deep state-space autoencoder for real-time raw speech enhancement. Processes waveforms end-to-end with no spectral pre/post-processing, outperforming prior real-time denoising models in PESQ while using only 0.84M parameters and 0.33G MACs. Remains performant even when input is compressed down to 4000 Hz and 4 bits.\nPLEIADES (NeurIPS 2025) — Uses structured temporal kernels based on orthogonal polynomials for online spatiotemporal classification and detection. Handles variable sample rates and discretization step-sizes without fine-tuning. Achieved state-of-the-art on DVS128 gesture recognition (99.6%), AIS 2024 eye tracking (99.6%), and PROPHESEE 1MP automotive detection (0.556 mAP) — all with sub-million parameter counts.\nOn the side, I also put together mamba-tiny — a minimal single-file PyTorch implementation of the Mamba SSM using logcumsumexp as an alternative to the parallel scan (which wasn\u0026rsquo;t available in PyTorch at the time).\nBack to projects\n","permalink":"https://peabrane.github.io/post/tenns/","summary":"\u003cp\u003eWhile working at \u003ca href=\"https://brainchip.com/\"\u003eBrainChip\u003c/a\u003e, I led the research and software-hardware codesign of TENNs (Temporal Neural Networks) — a family of deep state-space models designed for efficient, low-latency inference on edge hardware. This line of work developed in parallel with the broader SSM wave (Mamba, Gated Delta Net, etc.), cross-pollinating ideas around structured recurrences, parallel scans, and kernel parameterizations.\u003c/p\u003e","title":"Temporal Neural Networks (TENNs) at BrainChip"},{"content":"A mashup of Stay With Me and Golden Hour.\n","permalink":"https://peabrane.github.io/comps/stay_gold/","summary":"\u003cp\u003eA mashup of \u003ca href=\"https://www.youtube.com/watch?v=QNYT9wVwQ8A\"\u003e\u003cem\u003eStay With Me\u003c/em\u003e\u003c/a\u003e and \u003ca href=\"https://www.youtube.com/watch?v=PEM0Vs8jf1w\"\u003e\u003cem\u003eGolden Hour\u003c/em\u003e\u003c/a\u003e.\u003c/p\u003e","title":"Stay With Me x Golden Hour mashup"},{"content":"Unlike ferromagnetic spin models, a spin glass has a highly non-trivial phase transition profile. For mean field models (the SK spin glass), the glass experiences a replica-symmetry breaking phenomenon that is only fully understood recently. However, for short-range models, almost nothing is known rigorously, including basic problems such as the nature of the glass transition. A potential route towards settling such problems may rely on a suitable generalization of the random cluster model as a universal statistical tool.\nUsing a graphical representation for the Ising spin-glass known as the Chayes-Machta-Redner (CMR) representation combined with a contour argument, we were able to rigorously prove the existence of a finite-temperature phsae transition for the Ising spin-glass of geometric nature. The low-temperature phase is characterized by the coexistence of multiple infinite clusters that are separated spatially and negatively correlated with each other in their spin-overlaps, as shown in the figure below.\rBack to projects\n","permalink":"https://peabrane.github.io/post/glass/","summary":"\u003cp\u003eUnlike ferromagnetic spin models, a \u003ca href=\"https://en.wikipedia.org/wiki/Spin_glass\"\u003espin glass\u003c/a\u003e has a highly non-trivial phase transition profile. For mean field models (the SK spin glass), the glass experiences a \u003ca href=\"http://www.lptms.u-psud.fr/membres/mezard/Pdf/84_MPSTV_JDP.pdf\"\u003ereplica-symmetry breaking phenomenon\u003c/a\u003e that is only fully understood \u003ca href=\"https://arxiv.org/abs/1112.1003\"\u003erecently\u003c/a\u003e. However, for \u003ca href=\"https://arxiv.org/abs/cond-mat/9906076\"\u003eshort-range models\u003c/a\u003e, almost nothing is known \u003ca href=\"https://www.springer.com/gp/book/9783764357771\"\u003erigorously\u003c/a\u003e, including basic problems such as the nature of the glass transition. A potential route towards settling such problems may rely on a suitable generalization of the \u003ca href=\"https://en.wikipedia.org/wiki/Random_cluster_model\"\u003erandom cluster model\u003c/a\u003e as a \u003ca href=\"https://arxiv.org/abs/1711.02338\"\u003euniversal\u003c/a\u003e statistical tool.\u003c/p\u003e","title":"Phase Transition in Spin Glass"},{"content":"A lofi hip-hop made from samples of Richter\u0026rsquo;s recording on Scriabin\u0026rsquo;s piano sonata no.5.\n","permalink":"https://peabrane.github.io/comps/lofi/","summary":"\u003cp\u003eA \u003ca href=\"https://www.youtube.com/watch?v=Jys7Ibv7zqc\u0026amp;ab_channel=PeaBrane\"\u003elofi hip-hop\u003c/a\u003e made from samples of Richter\u0026rsquo;s recording on Scriabin\u0026rsquo;s \u003ca href=\"https://www.youtube.com/watch?v=xDTgj_69JKA\u0026amp;ab_channel=Pianoplayer002\"\u003epiano sonata no.5\u003c/a\u003e.\u003c/p\u003e","title":"An ecstatic lofi"},{"content":"A patchy sod with grass and wild floret Blue current spirals merging rocks and sand White smokes and droplets dawn the valleyed land Three puffs to lace the hazy mind forget\nCartwheel through time rolls back the wood roulette Sweet martingale each step we take is grand Through canyoned hills we surf yet where we strand In shrouded waves we rest with teared duvet\nBetween the steep ravine swirls gleamy stream Convening streams the withered fish shall spring\nUnder the dirge washed earth we search the verse To cry the spell, entomb the doom that looms In glassy mist the thirsted birch unearth With doomy gloom the fumy shrooms abloom\n","permalink":"https://peabrane.github.io/poems/tempo/","summary":"\u003cp\u003eA patchy sod with \u003cspan style=\"color: #4CAF50\"\u003egrass\u003c/span\u003e and wild floret \u003cbr\u003e\n\u003cspan style=\"color: #42a5f5\"\u003eBlue\u003c/span\u003e current spirals merging rocks and sand \u003cbr\u003e\n\u003cspan style=\"color: #e8e8e8\"\u003eWhite\u003c/span\u003e smokes and droplets dawn the valleyed land \u003cbr\u003e\nThree puffs to lace the hazy mind forget\u003c/p\u003e\n\u003cp\u003eCartwheel through time rolls back the \u003cspan style=\"color: #A0522D\"\u003ewood\u003c/span\u003e roulette \u003cbr\u003e\nSweet martingale each step we take is grand \u003cbr\u003e\nThrough canyoned hills we surf yet where we strand \u003cbr\u003e\nIn shrouded \u003cspan style=\"color: #5dade2\"\u003ewaves\u003c/span\u003e we rest with teared duvet\u003c/p\u003e\n\u003cp\u003eBetween the steep ravine swirls \u003cspan style=\"color: #FFD700\"\u003egleamy\u003c/span\u003e stream \u003cbr\u003e\nConvening streams the withered fish shall \u003cspan style=\"color: #66BB6A\"\u003espring\u003c/span\u003e\u003c/p\u003e","title":"Tempo"},{"content":"孤叶枝壳垂， 夕珠红缘坠。 朦雾残魂萧， 手瘦时噬泪。\nThis poem is about a maple leaf, supposedly always red.\n","permalink":"https://peabrane.github.io/poems/leaf/","summary":"\u003cp\u003e孤\u003cspan style=\"color: #e74c3c\"\u003e叶\u003c/span\u003e枝壳垂，\n夕珠\u003cspan style=\"color: #e74c3c\"\u003e红\u003c/span\u003e缘坠。\n朦\u003cspan style=\"color: #3498db\"\u003e雾\u003c/span\u003e残魂萧，\n手瘦时噬\u003cspan style=\"color: #3498db\"\u003e泪\u003c/span\u003e。\u003c/p\u003e","title":"枫水"},{"content":"白雀昏天何觅处， 黑鹰明空归残树。 心朱血青翼上溢， 黑泪白卷蜡灼数。\nThis poem is about a pair of ducks, one white and the other black.\nThe humming is the opening of Debussy\u0026rsquo;s Prelude Book 1 No. 12.\n","permalink":"https://peabrane.github.io/poems/black/","summary":"\u003cp\u003e\u003cspan style=\"color: #f0f0f0\"\u003e白\u003c/span\u003e雀昏天何觅处，\n\u003cspan style=\"color: #555\"\u003e黑\u003c/span\u003e鹰明空归残树。\n心\u003cspan style=\"color: #e74c3c\"\u003e朱\u003c/span\u003e血\u003cspan style=\"color: #00bcd4\"\u003e青\u003c/span\u003e翼上溢，\n\u003cspan style=\"color: #555\"\u003e黑\u003c/span\u003e泪\u003cspan style=\"color: #f0f0f0\"\u003e白\u003c/span\u003e卷蜡灼数。\u003c/p\u003e","title":"黑白"},{"content":"The universality classes of equilibrium phase transitions of many statistical models are well known. On the other hand, very little is understood about the phenomenon of non-equilibrium criticality, including its existence. Nonetheless, from numerical simulations, it appears that memory effects are fundamental driving forces behind dynamic criticality. They are capable of establishing long-range order irrespective of the structure of the equilibrium model. A theoretical understanding of the role of memory in complex systems is important to understanding critical phenomenon as a whole.\nWhen we endow memory to a spin glass, the dynamics will act to steadily decrease the effective temperature of the glass (left figure), in a regime that is continuously near dynamic criticality (the polynomial cluster size distribution in the right figure).\rBack to projects\n","permalink":"https://peabrane.github.io/post/memory/","summary":"\u003cp\u003eThe \u003ca href=\"https://en.wikipedia.org/wiki/Universality_class\"\u003euniversality classes\u003c/a\u003e of equilibrium phase transitions of many statistical models are well known. On the other hand, very little is understood about the phenomenon of \u003ca href=\"https://en.wikipedia.org/wiki/Critical_phenomena#Critical_dynamics\"\u003enon-equilibrium criticality\u003c/a\u003e, including its existence. Nonetheless, from \u003ca href=\"https://arxiv.org/abs/2102.04557\"\u003enumerical simulations\u003c/a\u003e, it appears that memory effects are fundamental driving forces behind dynamic criticality. They are capable of establishing long-range order irrespective of the structure of the equilibrium model. A theoretical understanding of the role of memory in complex systems is important to understanding critical phenomenon as a whole.\u003c/p\u003e","title":"Non-equilibrium Criticality of Memory"},{"content":"The title says it all. A short piano piece written in exactly 100 notes. Inspired by Nahre Sol\u0026rsquo;s 100 note challenge.\n","permalink":"https://peabrane.github.io/comps/opus_3/","summary":"\u003cp\u003eThe title says it all. A short \u003ca href=\"https://www.youtube.com/watch?v=ggRK0jN6nX0\u0026amp;ab_channel=PeaBrane\"\u003epiano piece\u003c/a\u003e written in exactly 100 notes. Inspired by Nahre Sol\u0026rsquo;s \u003ca href=\"https://www.youtube.com/watch?v=8GCNNDzqnHY\u0026amp;ab_channel=NahreSol\"\u003e100 note challenge\u003c/a\u003e.\u003c/p\u003e","title":"Opus 4: short piece with 100 notes"},{"content":"In classical economics, a potential offender is treated as a rational agent that seeks to maximize the expected utility when deciding whether to commit a crime. Therefore, if the perceived threat of legal punishment outweights the gains from the crime action, then the offender is deterred. However, in reality, there are complex behavioral traits governing the risk perception of each individual that deviate from rationality. These effects need to be captured in order to construct a general model of deterrence.\nBack to projects\n","permalink":"https://peabrane.github.io/post/deterrence/","summary":"\u003cp\u003eIn classical economics, a potential offender is treated as a \u003ca href=\"https://en.wikipedia.org/wiki/Rational_choice_theory\"\u003erational agent\u003c/a\u003e that seeks to maximize the expected utility when deciding whether to commit a crime. Therefore, if the perceived threat of legal punishment outweights the gains from the crime action, then the offender is \u003ca href=\"https://en.wikipedia.org/wiki/Deterrence_(penology)\"\u003edeterred\u003c/a\u003e. However, in reality, there are complex \u003ca href=\"https://en.wikipedia.org/wiki/Prospect_theory\"\u003ebehavioral traits\u003c/a\u003e governing the risk perception of each individual that deviate from rationality. These effects need to be captured in order to construct a \u003ca href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3457220\"\u003egeneral model of deterrence\u003c/a\u003e.\u003c/p\u003e","title":"Optimal Deterrence Based on Behavioral Traits"},{"content":"A happy birthday incorporating all of my wife Yuki\u0026rsquo;s favorite songs as her birthday present.\n","permalink":"https://peabrane.github.io/comps/happy_birthday/","summary":"\u003cp\u003eA happy birthday incorporating all of my wife Yuki\u0026rsquo;s favorite songs as her birthday present.\u003c/p\u003e","title":"Opus 3: Happy Birthday — a medley of her favorite songs"},{"content":"On a signed graph (or spin glass), often times there are interactions that cannot be satisfied regardless of how one assigns the spin states. This property is known as frustration. The study of frustration of a bipartite graph is an active topic in graph theory, and has some important applications in unsupervised learning. For instance, a highly frustrated RBM has several desirable statistical properties, such as the correspondence between the joint and marginal distribution, that can be leveraged to achieve more efficient training.\nThere is some numerical evidence that the complexity of a bipartite glass (or RBM) may display a double phase-transition as the frustration ratio is increased.\nThe frustration ratio may also be used as a proxy (in place of KL-divergence) in monitoring the training progress of the RBM, as it is expected to be more easily estimated.\nBack to projects\n","permalink":"https://peabrane.github.io/post/rbm/","summary":"\u003cp\u003eOn a \u003ca href=\"https://www.sciencedirect.com/science/article/pii/0166218X82900336\"\u003esigned graph\u003c/a\u003e (or spin glass), often times there are interactions that cannot be satisfied regardless of how one assigns the spin states. This property is known as frustration. The study of \u003ca href=\"https://www.combinatorics.org/ojs/index.php/eljc/article/view/v19i4p10\"\u003efrustration of a bipartite graph\u003c/a\u003e is an active topic in graph theory, and has some important applications in unsupervised learning. For instance, a \u003ca href=\"https://jmlr.org/papers/v21/19-368.html\"\u003ehighly frustrated RBM\u003c/a\u003e has several desirable statistical properties, such as the correspondence between the joint and marginal distribution, that can be leveraged to achieve more \u003ca href=\"https://arxiv.org/abs/2001.05559\"\u003eefficient training\u003c/a\u003e.\u003c/p\u003e","title":"Frustration and Unsupervised Learning"},{"content":"A playing of Debussy\u0026rsquo;s La plus que lente set over travel pics to Tohoku.\n","permalink":"https://peabrane.github.io/comps/debussy_tohoku/","summary":"\u003cp\u003eA playing of Debussy\u0026rsquo;s \u003cem\u003eLa plus que lente\u003c/em\u003e set over travel pics to Tohoku.\u003c/p\u003e","title":"Debussy: La plus que lente over Tohoku"},{"content":"A fantasy inspired by a trip to Gokayama. Incorporates elements of impressionism, Hirajoshi scale, mysticism, and jazz into a musical porridge to be consumed in 3 minutes.\n","permalink":"https://peabrane.github.io/comps/opus_2/","summary":"\u003cp\u003eA fantasy inspired by a trip to \u003ca href=\"https://www.japan-guide.com/e/e5950.html\"\u003eGokayama\u003c/a\u003e. Incorporates elements of impressionism, Hirajoshi scale, mysticism, and jazz into a musical porridge to be \u003ca href=\"https://www.youtube.com/watch?v=KIa9ACeGUkg\u0026amp;ab_channel=PeaBrane\"\u003econsumed\u003c/a\u003e in 3 minutes.\u003c/p\u003e","title":"Opus 2: Green Water"},{"content":"An early attempt to write a short prelude in a Prokofievian style. The title is almost a joke because the bulk of this piece is not in C.\n","permalink":"https://peabrane.github.io/comps/opus_1/","summary":"\u003cp\u003eAn early attempt to write a short prelude in a \u003ca href=\"https://en.wikipedia.org/wiki/Sergei_Prokofiev\"\u003eProkofievian\u003c/a\u003e style. The title is almost a joke because the bulk of this piece is not in C.\u003c/p\u003e","title":"Opus 1: prelude in C"},{"content":"I try to be learnt.\nPhysics PhD with CS specialization @ UCSD Research @ MIT in Wolfgang Ketterle\u0026rsquo;s (Nobel laureate) Dypole lab Physics + Math @ UCLA (summa cum laude, done in 2 years) ","permalink":"https://peabrane.github.io/cv/","summary":"\u003cp\u003eI try to be learnt.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePhysics PhD\u003c/strong\u003e with CS specialization @ UCSD\u003c/li\u003e\n\u003cli\u003eResearch @ MIT in Wolfgang Ketterle\u0026rsquo;s (Nobel laureate) \u003ca href=\"https://dypole.mit.edu/\"\u003eDypole lab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePhysics + Math\u003c/strong\u003e @ UCLA (summa cum laude, done in 2 years)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cembed src=\"/cv/pei_cv.pdf\" type=\"application/pdf\" width=\"100%\" height=\"800px\" /\u003e","title":"CV"}]